{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DistilBertTokenizer, RobertaTokenizer, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random \n",
    "import sys\n",
    "import math \n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import random \n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import math, copy, time \n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"roberta-base\"\n",
    "MAX_LENGTH = 128\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "\n",
    "USE_ATT = True\n",
    "\n",
    "BERT_DROPOUT = 0.2\n",
    "LSTM_UNITS = 50\n",
    "DENSE_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Transformer model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=MAX_LENGTH,\n",
    "                                       pad_to_max_length=True,\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])\n",
    "\n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(\n",
    "        input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL,\n",
    "                                                do_lower_case=True,\n",
    "                                                add_special_tokens=True,\n",
    "                                                truncation=True,\n",
    "                                                max_length=MAX_LENGTH,\n",
    "                                                padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:  train (4843,) test (1211,)\n",
      "SVO:  train (4843, 3, 128) test (1211, 3, 128)\n",
      "Profanity:  train (4843,) test (1211,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4843 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/karet/Documents/IRE/Project/hatetext-socialmedia/.venv/lib64/python3.11/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 4843/4843 [00:01<00:00, 2689.20it/s]\n",
      "100%|██████████| 1211/1211 [00:00<00:00, 3488.36it/s]\n"
     ]
    }
   ],
   "source": [
    "SVO_P_FILE = './data/hate_int_prof_SVO.tsv'\n",
    "SVO_P_data = pd.read_csv(SVO_P_FILE, sep='\\t')\n",
    "SVO_P_data[['Subject', 'Verb', 'Object']] = SVO_P_data[['Subject', 'Verb', 'Object']].applymap(lambda x: np.array(eval(x)))\n",
    "SVO_P_data['SVO'] = SVO_P_data.apply(lambda row: np.row_stack((row['Subject'], row['Verb'], row['Object'])), axis=1)\n",
    "\n",
    "sentences = SVO_P_data['Sentence'].to_numpy()\n",
    "hate_intensities = SVO_P_data['Intensity'].to_numpy()\n",
    "profanity = SVO_P_data['Profanity'].to_numpy()\n",
    "# SVO labelled after using roberta base tokenizer\n",
    "SVO = SVO_P_data['SVO'].to_numpy()\n",
    "\n",
    "def padd_array_with_zeros(arr, desired_len):\n",
    "    # Prepend 0 to accomodate BERT [CLS] token\n",
    "    arr = np.insert(arr, 0, 0)\n",
    "    # Padding\n",
    "    current_len = len(arr)\n",
    "    if current_len < desired_len:\n",
    "        padded_arr = np.pad(arr, (0, desired_len - current_len), mode='constant')\n",
    "    else:\n",
    "        padded_arr = arr[:desired_len]\n",
    "\n",
    "    return padded_arr\n",
    "\n",
    "inp = list(zip(sentences, SVO, profanity))\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(inp, hate_intensities,\n",
    "                                            test_size=0.2, random_state=78)\n",
    "\n",
    "train_sentences = np.array([t[0] for t in X_tr])\n",
    "train_SVO = [t[1] for t in X_tr]\n",
    "train_profanity = np.array([t[2] for t in X_tr])\n",
    "\n",
    "test_sentences = np.array([t[0] for t in X_te])\n",
    "test_SVO = [t[1] for t in X_te]\n",
    "test_profanity = np.array([t[2] for t in X_te])\n",
    "\n",
    "## Padding zeros to SVO to make all of them same length\n",
    "train_SVO_padded = list()\n",
    "for sample in train_SVO:\n",
    "    train_SVO_padded.append([padd_array_with_zeros(arr, MAX_LENGTH) for arr in sample])\n",
    "train_SVO = np.array(train_SVO_padded)\n",
    "\n",
    "test_SVO_padded = list()\n",
    "for sample in test_SVO:\n",
    "    test_SVO_padded.append([padd_array_with_zeros(arr, MAX_LENGTH) for arr in sample])\n",
    "test_SVO = np.array(test_SVO_padded)\n",
    "\n",
    "print('Sentences: ', 'train', train_sentences.shape, 'test', test_sentences.shape)\n",
    "print('SVO: ', 'train', train_SVO.shape, 'test', test_SVO.shape)\n",
    "print('Profanity: ', 'train', train_profanity.shape, 'test', test_profanity.shape)\n",
    "train_SVO = torch.from_numpy(train_SVO).float()\n",
    "test_SVO = torch.from_numpy(test_SVO).float()\n",
    "train_SVO = train_SVO.transpose(1, 2)\n",
    "test_SVO = test_SVO.transpose(1, 2)\n",
    "\n",
    "train_input_ids, train_input_masks, train_input_segment = tokenize(\n",
    "    train_sentences, tokenizer)\n",
    "test_input_ids, test_input_masks, test_input_segment = tokenize(\n",
    "    test_sentences, tokenizer)\n",
    "y_tr = np.asarray(y_tr)\n",
    "y_te = np.asarray(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_index(relative_position, alpha, beta, gamma, dtype):\n",
    "    rp_abs = relative_position.abs()\n",
    "    mask = rp_abs <= alpha\n",
    "    not_mask = ~mask\n",
    "    rp_out = relative_position[not_mask]\n",
    "    rp_abs_out = rp_abs[not_mask]\n",
    "    y_out = (torch.sign(rp_out)* (alpha +\n",
    "                                 torch.log(rp_abs_out/alpha)/\n",
    "                                 math.log(gamma/alpha)*\n",
    "                                 (beta - alpha)).round().clip(max=beta)).to(dtype)\n",
    "    idx = relative_position.clone()\n",
    "    if idx.dtype in [torch.float32,torch.float64]:\n",
    "        idx = idx.round().to(dtype)\n",
    "\n",
    "    idx[not_mask] = y_out\n",
    "    return idx\n",
    "\n",
    "class PositionalEncoding2(nn.Module):\n",
    "    def __init__(self, n_functions):\n",
    "        super(PositionalEncoding2, self).__init__()\n",
    "\n",
    "        self.register_buffer('frequencies', 2.0 ** torch.arange(n_functions))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [..., dim]\n",
    "\n",
    "        Returns:\n",
    "            embedding: a temporal embedding of `x` of shape [..., n_functions * dim * 2]\n",
    "        \"\"\"\n",
    "        freq = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
    "\n",
    "        embedding = torch.zeros(*freq.shape[:-1], freq.shape[-1] * 2).cuda()\n",
    "        embedding[..., 0::2] = freq.sin()\n",
    "        embedding[..., 1::2] = freq.cos()\n",
    "\n",
    "        return embedding\n",
    "\n",
    "class SirenBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, w0=30, c=6, is_first=False, use_bias=True, activation=None):\n",
    "        super(SirenBlock, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.is_first = is_first\n",
    "\n",
    "        weight = torch.zeros(out_features, in_features)\n",
    "        bias = torch.zeros(out_features) if use_bias else None\n",
    "        self.init(weight, bias, c=c, w0=w0)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = nn.Parameter(bias) if use_bias else None\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def init(self, weight, bias, c, w0):\n",
    "        n = self.in_features\n",
    "\n",
    "        w_std = (1 / n) if self.is_first else (np.sqrt(c / n) / w0)\n",
    "        weight.uniform_(-w_std, w_std)\n",
    "\n",
    "        if bias is not None:\n",
    "            bias.uniform_(-w_std, w_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.linear(x, self.weight, self.bias)\n",
    "\n",
    "        return self.activation(out)\n",
    "\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, norm_layer=False, activation=None):\n",
    "        super(FCBlock, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.residual = (in_features == out_features)  # when the input and output have the same dimensions, build a residual block\n",
    "        self.norm_layer = nn.LayerNorm(out_features) if norm_layer else None\n",
    "        self.activation = nn.ReLU(inplace=True) if activation is None else activation\n",
    "        dropout = 0.1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, D), features are in the last dimension.\n",
    "        \"\"\"\n",
    "        out = self.fc(x)\n",
    "\n",
    "        if self.norm_layer is not None:\n",
    "            out = self.norm_layer(out)\n",
    "\n",
    "        if self.residual:\n",
    "            return self.activation(out) + x\n",
    "\n",
    "        # return self.activation(out)\n",
    "        return self.dropout(self.activation(out))\n",
    "\n",
    "class EncoderDecoder2(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many\n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder2, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # self.fl1 = nn.Linear(512,512)\n",
    "        # self.fl = nn.Linear(512*2,18*3*2)\n",
    "        self.fl = nn.Linear(128*128,1)\n",
    "        # d_model = 256\n",
    "        # self.inskele = 100\n",
    "        # self.ingli = 100\n",
    "        # self.convskele = nn.Sequential(nn.Conv1d(in_channels=self.inskele, out_channels=d_model, kernel_size=1, bias=False),\n",
    "        #                            nn.ReLU(),\n",
    "        #                            nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=6, bias=False),\n",
    "        #                            nn.ReLU(),\n",
    "        #                            nn.Conv1d(in_channels=d_model, out_channels=self.ingli, kernel_size=5, bias=False),\n",
    "        #                            nn.ReLU())\n",
    "        # self.convgli = nn.Sequential(nn.Conv1d(in_channels=self.ingli, out_channels=d_model, kernel_size=1, bias=False),\n",
    "        #                            nn.ReLU(),\n",
    "        #                            nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=6, bias=False),\n",
    "        #                            nn.ReLU(),\n",
    "        #                            nn.Conv1d(in_channels=d_model, out_channels=self.ingli, kernel_size=5, bias=False),\n",
    "        #                            nn.ReLU())\n",
    "\n",
    "        self.bandwidth = 7\n",
    "        if self.bandwidth != 0:\n",
    "            # print(self.bandwidth,\"BANDWITH\")\n",
    "            self.positional_encoding = PositionalEncoding2(self.bandwidth)\n",
    "            embedding_dim = self.bandwidth * 2\n",
    "        else:\n",
    "            embedding_dim = 1\n",
    "\n",
    "        global_z = 50\n",
    "        hidden_neuron = 256\n",
    "        local_z = 18*3 + 15\n",
    "        skip_connection = True\n",
    "        siren = False\n",
    "        norm_layer = True\n",
    "        hidden_neuron = hidden_neuron\n",
    "        in_features = 1 + local_z if siren else embedding_dim + local_z\n",
    "        local_in_features = hidden_neuron + in_features if skip_connection else hidden_neuron\n",
    "        global_in_features = local_in_features + global_z if skip_connection else hidden_neuron\n",
    "        local_output = 50\n",
    "\n",
    "        layers = [\n",
    "            SirenBlock(in_features, hidden_neuron, is_first=True) if siren else FCBlock(in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(global_in_features, hidden_neuron) if siren else FCBlock(global_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(global_in_features, hidden_neuron) if siren else FCBlock(global_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "            SirenBlock(global_in_features, hidden_neuron) if siren else FCBlock(global_in_features, hidden_neuron, norm_layer=norm_layer)\n",
    "        ]\n",
    "        # layers = [\n",
    "        #     SirenBlock(in_features, hidden_neuron, is_first=True) if siren else FCBlock(in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "        #     SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "        #     SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "        #     SirenBlock(local_in_features, hidden_neuron) if siren else FCBlock(local_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "        #     SirenBlock(global_in_features, hidden_neuron) if siren else FCBlock(global_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "        #     SirenBlock(global_in_features, hidden_neuron) if siren else FCBlock(global_in_features, hidden_neuron, norm_layer=norm_layer),\n",
    "        #     SirenBlock(global_in_features, hidden_neuron) if siren else FCBlock(global_in_features, hidden_neuron, norm_layer=norm_layer)\n",
    "        # ]\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "        self.skip_layers = [] if not skip_connection else list(range(1, len(self.mlp)))\n",
    "        self.local_layers = list(range(8))\n",
    "        # self.local_layers = list(range(4))\n",
    "        self.local_linear = nn.Sequential(nn.Linear(hidden_neuron, local_output))\n",
    "\n",
    "        # w = 100\n",
    "        # self.dctstackdec = []\n",
    "        # self.dctstackdec.append(0)\n",
    "        # for w in range(1,101):\n",
    "        #     self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "        #     # self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "        #     self.idct_m = torch.from_numpy(self.idct_m).float().cuda()\n",
    "        #     self.dctstackdec.append(self.idct_m)\n",
    "\n",
    "    # def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "    #     \"Take in and process masked src and target sequences.\"\n",
    "    #     y =self.decode(self.encode(src, src_mask), src_mask,\n",
    "    #                         tgt, tgt_mask)\n",
    "    #     # y = self.fl1(y)\n",
    "    #     # y = self.relu(y)\n",
    "    #     y = self.fl(y)\n",
    "\n",
    "    #     # q,w,e = y.shape\n",
    "\n",
    "    #     # self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "    #     # self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "    #     # self.idct_m = torch.from_numpy(self.idct_m).float().cuda()\n",
    "    #     # y = torch.matmul(self.idct_m,y)\n",
    "    #     # y = torch.matmul(self.dctstackdec[w],y)\n",
    "    #     return y\n",
    "\n",
    "    def forward(self, src, relenc, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "\n",
    "\n",
    "\n",
    "        y = self.encode(src,relenc ,src_mask)\n",
    "        a,b,c = y.shape\n",
    "        y = y.reshape(a,b*c)\n",
    "        # y =self.decode(self.encode(src,relenc ,src_mask), src_mask,\n",
    "        #                     tgt, tgt_mask)\n",
    "        # y = self.fl1(y)\n",
    "        # y = self.relu(y)\n",
    "        y = self.fl(y)\n",
    "\n",
    "        # q,w,e = y.shape\n",
    "\n",
    "        # self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "        # self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "        # self.idct_m = torch.from_numpy(self.idct_m).float().cuda()\n",
    "        # y = torch.matmul(self.idct_m,y)\n",
    "        # y = torch.matmul(self.dctstackdec[w],y)\n",
    "        print(y.shape,\"Y SHAPE\")\n",
    "        return y\n",
    "    # def encode(self, src, src_mask):\n",
    "        # return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def encode(self, src, relenc,src_mask):\n",
    "        return self.encoder(self.src_embed(src),relenc,src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder2(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    # def forward(self, x, mask):\n",
    "    #     \"Pass the input (and mask) through each layer in turn.\"\n",
    "    #     for layer in self.layers:\n",
    "    #         x = layer(x, mask)\n",
    "    #     return self.norm(x)\n",
    "\n",
    "    def forward(self, x, relenc,mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, relenc, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         \"Follow Figure 1 (left) for connections.\"\n",
    "# #         print(x.shape,\"X SHAPE HEJRE\")\n",
    "#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "#         return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "    def forward(self, x,relenc, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "#         print(x.shape,\"X SHAPE HEJRE\")\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, relenc,mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Decoder2(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder2, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "#         self.fl = nn.Linear(512,108)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "#         print(x.shape,\"X OUT DEC SHAPE\")\n",
    "#         print(x.dtype,\"X DTYPE HERE\")\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.imshow(subsequent_mask(20)[0])\n",
    "# None\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "#         print(scores.shape,\"scores\")\n",
    "#         print(mask.shape,\"mask shape\")\n",
    "#         exit()\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    # print(scores.shape,\"SCORES SHAPE HERE\")\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "def attentionrel(query, key, value,relencq,relenck,relencv,mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    a =torch.matmul(query,relencq.transpose(-2,-1))\n",
    "    # print(a.shape,\"A SHAPE\")\n",
    "    b = torch.matmul(relenck,key.transpose(-2,-1))\n",
    "    # print(b.shape,\"B SHAPE\")\n",
    "    pos_bias = a + b\n",
    "    # global flagenc\n",
    "    # global lastposbias\n",
    "    # flagenc = flagenc + 1\n",
    "    # if(flagenc==600):\n",
    "    #     lastposbias = pos_bias\n",
    "    #     lastposbias.retain_grad()\n",
    "    # print(flagenc,\"FLAGENC HERE CHECK IT OUT\")\n",
    "    # pos_bias = torch.matmul(query,relencq) + torch.matmul(relenck,key.transpose(-2,-1))\n",
    "    # temp = (torch.matmul(query, key.transpose(-2, -1)) + pos_bias) \\\n",
    "            #  / math.sqrt(d_k)\n",
    "    # print(temp.shape,\"TEMP SHAPE\")\n",
    "    # exit()\n",
    "    # scores = torch.matmul((query, key.transpose(-2, -1))) \\\n",
    "            #  / math.sqrt(d_k)\n",
    "    scores = (torch.matmul(query, key.transpose(-2, -1)) + pos_bias) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "#         print(scores.shape,\"scores\")\n",
    "#         print(mask.shape,\"mask shape\")\n",
    "#         exit()\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    # print(scores.shape,\"SCORES SHAPE HERE\")\n",
    "    # exit()\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # print(torch.matmul(p_attn, value).shape,\"FINAL SHAPE HERE\")\n",
    "    # exit()\n",
    "    return torch.matmul(p_attn, value+relencv), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "#         print(query.shape,key.shape,value.shape,\"before linear trans\")\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "#         print(query.shape,key.shape,value.shape,mask.shape,\"attention stuff here\")\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "#         print(query.shape,key.shape,value.shape,mask.shape,\"later attention stuff here\")\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class MultiHeadedAttentionRel(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttentionRel, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # self.relfeat = 15*15\n",
    "        self.relfeat = 3\n",
    "        self.linearrelposq = nn.Linear(self.relfeat,d_model)\n",
    "        self.linearrelposk = nn.Linear(self.relfeat,d_model)\n",
    "        self.linearrelposv = nn.Linear(self.relfeat,d_model)\n",
    "\n",
    "    def forward(self, query, key, value,relenc, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "#         print(query.shape,key.shape,value.shape,\"before linear trans\")\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        # print(query.shape,\"QUERY SHAPE\")\n",
    "        # print(relenc.shape,\"RELENC SHAPE\")\n",
    "        relencq = self.linearrelposq(relenc).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        relenck = self.linearrelposk(relenc).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        relencv = self.linearrelposv(relenc).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # print(relencq.shape,\"RELENCqqq   SHAPE\")\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "#         print(query.shape,key.shape,value.shape,mask.shape,\"attention stuff here\")\n",
    "        x, self.attn = attentionrel(query, key, value,relencq,relenck,relencv, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "#         print(query.shape,key.shape,value.shape,mask.shape,\"later attention stuff here\")\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "#         print(vocab,\"VOCAB SIZE\")\n",
    "#         self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.lut1 = nn.Linear(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape,\"EMBEDDING X SHAPE\")\n",
    "#         print(x.dtype,\"EMBEDDING TYPE\")\n",
    "#         y = self.lut(x) * math.sqrt(self.d_model)\n",
    "        y = self.lut1(x)*math.sqrt(self.d_model)\n",
    "#         print(y.shape,\"EMBEDDING Y SHAPE\")\n",
    "        return y\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "#         print(pe[:,0::2].shape)\n",
    "#         print(pe[:,1::2].shape)\n",
    "#         print(pe.shape,\"PE shape\")\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "#         (print(pe.shape))\n",
    "        self.register_buffer('pe', pe)\n",
    "        # w = 100\n",
    "        # self.dctstack = []\n",
    "        # self.dctstack.append(0)\n",
    "        # for w in range(1,101):\n",
    "        #     self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "        #     self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "        #     self.dctstack.append(self.dct_m)\n",
    "        # self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "        # self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "        # self.idct_m = torch.from_numpy(self.idct_m).float().cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape,\"XSHAPE\")\n",
    "#         print(Variable(self.pe[:, :x.size(1)],\n",
    "#                          requires_grad=False).repeat(1,1,108).shape,\"pe shape\")\n",
    "        a,b,c = Variable(self.pe[:, :x.size(1)],\n",
    "                         requires_grad=False).repeat(1,1,18*3).shape\n",
    "#         print(x.shape,\"x pos shape\")\n",
    "#         x = x + Variable(self.pe[:, :x.size(1)],\n",
    "#                          requires_grad=False).repeat(1,1,108).reshape(a,b,108,-1)\n",
    "        q,w,e = x.shape\n",
    "        # self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "        # self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "        # self.idct_m = torch.from_numpy(self.idct_m).float().cuda()\n",
    "        # x = torch.matmul(self.dct_m,x)\n",
    "        x = x + Variable(self.pe[:, :x.size(1)],\n",
    "                         requires_grad=False)\n",
    "        # self.dct_m, self.idct_m = util.get_dct_matrix(w) # (20, 20)\n",
    "        # self.dct_m = torch.from_numpy(self.dct_m).float().cuda()\n",
    "        # self.idct_m = torch.from_numpy(self.idct_m).float().cuda()\n",
    "        # x = torch.matmul(self.dct_m,x)\n",
    "        # print(w,\"W VALUE\")\n",
    "        # x = torch.matmul(self.dctstack[w],x)\n",
    "        # x = x\n",
    "        # print(x.shape,\"x pos hspae\")\n",
    "        return self.dropout(x)\n",
    "\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# pe = PositionalEncoding(20, 0)\n",
    "# y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "# print(y.shape)\n",
    "# plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "# plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "# None\n",
    "\n",
    "def make_modeltrans(src_vocab, tgt_vocab, N=6,\n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "\n",
    "    d_model=128\n",
    "    h=4\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    attnRel = MultiHeadedAttentionRel(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # model = EncoderDecoder2(\n",
    "    #     Encoder2(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "    #     Decoder2(DecoderLayer(d_model, c(attn), c(attn),\n",
    "    #                          c(ff), dropout), N),\n",
    "    #     # nn.Sequential(Embeddings(d_model, src_vocab+7+36+15+15), c(position)),\n",
    "    #     nn.Sequential(Embeddings(d_model, src_vocab+7+15), c(position)),\n",
    "    #     nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "    #     Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # model = EncoderDecoder2(\n",
    "    #     Encoder2(EncoderLayer(d_model, c(attnRel), c(ff), dropout), N),\n",
    "    #     Decoder2(DecoderLayer(d_model, c(attn), c(attn),\n",
    "    #                          c(ff), dropout), N),\n",
    "    #     # nn.Sequential(Embeddings(d_model, src_vocab+7+36+15+15), c(position)),\n",
    "    #     nn.Sequential(Embeddings(d_model, 58), c(position)),\n",
    "    #     nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "    #     Generator(d_model, tgt_vocab))\n",
    "\n",
    "    model = EncoderDecoder2(\n",
    "        Encoder2(EncoderLayer(d_model, c(attnRel), c(ff), dropout), N),\n",
    "        Decoder2(DecoderLayer(d_model, c(attn), c(attn),\n",
    "                             c(ff), dropout), N),\n",
    "        # nn.Sequential(Embeddings(d_model, src_vocab+7+36+15+15), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, 768), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/karet/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoder2(\n",
      "  (encoder): Encoder2(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (self_attn): MultiHeadedAttentionRel(\n",
      "          (linears): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linearrelposq): Linear(in_features=3, out_features=128, bias=True)\n",
      "          (linearrelposk): Linear(in_features=3, out_features=128, bias=True)\n",
      "          (linearrelposv): Linear(in_features=3, out_features=128, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0-1): 2 x SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm()\n",
      "  )\n",
      "  (decoder): Decoder2(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0-2): 3 x SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm()\n",
      "  )\n",
      "  (src_embed): Sequential(\n",
      "    (0): Embeddings(\n",
      "      (lut1): Linear(in_features=768, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embed): Sequential(\n",
      "    (0): Embeddings(\n",
      "      (lut1): Linear(in_features=108, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (proj): Linear(in_features=128, out_features=108, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (fl): Linear(in_features=16384, out_features=1, bias=True)\n",
      "  (positional_encoding): PositionalEncoding2()\n",
      "  (mlp): ModuleList(\n",
      "    (0): FCBlock(\n",
      "      (fc): Linear(in_features=83, out_features=256, bias=True)\n",
      "      (norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1-7): 7 x FCBlock(\n",
      "      (fc): Linear(in_features=339, out_features=256, bias=True)\n",
      "      (norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8-10): 3 x FCBlock(\n",
      "      (fc): Linear(in_features=389, out_features=256, bias=True)\n",
      "      (norm_layer): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (local_linear): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36011/1908745257.py:626: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "def make_std_mask(tgt, pad):\n",
    "  \"Create a mask to hide padding and future words.\"\n",
    "  tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "  tgt_mask = tgt_mask & Variable(\n",
    "      subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "  return tgt_mask\n",
    "\n",
    "# Roberta base model \n",
    "with torch.no_grad():\n",
    "  model3 = torch.hub.load('huggingface/pytorch-transformers', 'model', 'roberta-base')\n",
    "\n",
    "# Intialising custom transformer\n",
    "V = 18*3*2\n",
    "model_trans = make_modeltrans(V, V, N=6).cpu()\n",
    "model_opt = torch.optim.Adam(model_trans.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 \n",
    "batch_size = 15\n",
    "batch, b = train_input_ids.shape\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for part in range(int(batch/batch_size)-1):\n",
    "\n",
    "    input_ids_in, input_masks_in = train_input_ids[part*batch_size:(part+1)*batch_size], train_input_masks[part*batch_size:(part+1)*batch_size]\n",
    "    train_SVO_in = train_SVO[part*batch_size:(part+1)*batch_size] # this is SVO encoding\n",
    "    #print(input_ids_in.shape,input_masks_in.shape)\n",
    "\n",
    "    input_y = y_tr[part*batch_size:(part+1)*batch_size]\n",
    "    input_ids_in, input_masks_in, input_ids_in = torch.tensor(input_ids_in), torch.tensor(input_masks_in), torch.tensor(input_ids_in)\n",
    "\n",
    "    # Get embeddings from roberta base\n",
    "    embeddings = model3(input_ids_in, input_masks_in)[0]\n",
    "\n",
    "    a,b,c = embeddings.shape\n",
    "    src = torch.randn(a,b)\n",
    "    pad = 0\n",
    "    src_mask = (src != pad).unsqueeze(-2).cpu()\n",
    "    dtrg = src\n",
    "    dtrg_y = src[:,1:]\n",
    "\n",
    "    trg_mask = make_std_mask(dtrg, pad).cpu()\n",
    "    i = 127\n",
    "    A_traind = embeddings.cpu()\n",
    "    Bpad = torch.ones(a,i+1,18*3).cpu()\n",
    "    relenc = train_SVO_in # this is input of svo, size B x 128 x 3\n",
    "    dtrg = src[:,:i+1]\n",
    "    trg_mask = make_std_mask(dtrg, pad).cpu()\n",
    "\n",
    "    out = model_trans.forward(A_traind,relenc,Bpad,src_mask,trg_mask)\n",
    "    input_y = torch.tensor(input_y).cpu().float()\n",
    "    loss = criterion(out,input_y)\n",
    "    print(\"loss value\",loss.item())\n",
    "\n",
    "    model_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    model_opt.step()\n",
    "  if(epoch%3==0):\n",
    "    torch.save(model_trans.state_dict(),\"./svotransformer\"+str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, b = test_input_ids.shape\n",
    "batch_size = 7\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for part in tqdm(range(int(batch/batch_size)-1)):\n",
    "\n",
    "    input_ids_in, input_masks_in = test_input_ids[part*batch_size:(part+1)*batch_size], test_input_masks[part*batch_size:(part+1)*batch_size]\n",
    "    test_SVO_in = test_SVO[part*batch_size:(part+1)*batch_size] # this is SVO encoding\n",
    "\n",
    "    input_y = y_te[part*batch_size:(part+1)*batch_size]\n",
    "    input_ids_in, input_masks_in, input_ids_in = torch.tensor(input_ids_in), torch.tensor(input_masks_in), torch.tensor(input_ids_in)\n",
    "\n",
    "    pred = model3(input_ids_in, input_masks_in)[0]\n",
    "\n",
    "    data_ = pred\n",
    "    a,b,c = data_.shape\n",
    "    src = torch.randn(a,b)\n",
    "    pad = 0\n",
    "    src_mask = (src != pad).unsqueeze(-2).cpu()\n",
    "    dtrg = src\n",
    "    dtrg_y = src[:,1:]\n",
    "\n",
    "    trg_mask = make_std_mask(dtrg, pad).cpu()\n",
    "    i = 127\n",
    "    A_testd = data_.cpu()\n",
    "    Bpad = torch.ones(a,i+1,18*3).cpu()\n",
    "    relenc = test_SVO_in.cpu() # this is input of svo, size B x 128 x 3\n",
    "    dtrg = src[:,:i+1]\n",
    "    trg_mask = make_std_mask(dtrg, pad).cpu()\n",
    "\n",
    "    out = model_trans.forward(A_testd,relenc,Bpad,src_mask,trg_mask)\n",
    "\n",
    "    all_predictions.append(out.cpu().numpy())\n",
    "    all_true_labels.append(input_y)\n",
    "\n",
    "all_predictions = np.array([item for sublist in all_predictions for item in sublist]).flatten()\n",
    "all_true_labels = np.array([item for sublist in all_true_labels for item in sublist])\n",
    "all_predictions_ro = np.round(all_predictions)\n",
    "\n",
    "rmse = mean_squared_error(all_true_labels, all_predictions_ro, squared=False)\n",
    "print(\"RMSE:\", rmse)\n",
    "pearson_corr, _ = stats.pearsonr(all_true_labels, all_predictions_ro)\n",
    "print(\"Pearson correlation:\", pearson_corr)\n",
    "cosine_sim = 1 - distance.cosine(all_true_labels, all_predictions_ro)\n",
    "print(\"Cosine similarity:\", cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
